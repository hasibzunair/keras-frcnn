{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras_frcnn import config, data_generators\n",
    "from keras_frcnn import losses as losses\n",
    "import keras_frcnn.roi_helpers as roi_helpers\n",
    "from keras.utils import generic_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras_frcnn.pascal_voc_parser import get_data\n",
    "from keras_frcnn import vgg as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_path = 'drive/My Drive/AI/Faster_RCNN'\n",
    "base_path = \"/home/hasib/keras-frcnn/\"\n",
    "#train_path =  'drive/My Drive/AI/Dataset/Open Images Dataset v4 (Bounding Boxes)/person_car_phone_train_annotation.txt' # Training data (annotation file)\n",
    "train_path = \"datasets/\"\n",
    "\n",
    "num_rois = 4 # Number of RoIs to process at once.\n",
    "\n",
    "# Augmentation flag\n",
    "horizontal_flips = True # Augment with horizontal flips in training. \n",
    "vertical_flips = True   # Augment with vertical flips in training. \n",
    "rot_90 = True           # Augment with 90 degree rotations in training. \n",
    "\n",
    "output_weight_path = os.path.join(base_path, 'model/model_frcnn_vgg_exp1.hdf5')\n",
    "\n",
    "record_path = os.path.join(base_path, 'model/record.csv') # Record data (used to save the losses, classification accuracy and mean average precision)\n",
    "\n",
    "base_weight_path = os.path.join(base_path, 'model/vgg16.h5')\n",
    "\n",
    "config_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config\n",
    "C = config.Config()\n",
    "\n",
    "C.use_horizontal_flips = horizontal_flips\n",
    "C.use_vertical_flips = vertical_flips\n",
    "C.rot_90 = rot_90\n",
    "\n",
    "C.record_path = record_path\n",
    "C.model_path = output_weight_path\n",
    "C.num_rois = num_rois\n",
    "C.base_net_weights = base_weight_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n"
     ]
    }
   ],
   "source": [
    "all_imgs, classes_count, class_mapping = get_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17125"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images per class:\n",
      "{'aeroplane': 1002,\n",
      " 'bg': 0,\n",
      " 'bicycle': 837,\n",
      " 'bird': 1271,\n",
      " 'boat': 1059,\n",
      " 'bottle': 1561,\n",
      " 'bus': 685,\n",
      " 'car': 2492,\n",
      " 'cat': 1277,\n",
      " 'chair': 3056,\n",
      " 'cow': 771,\n",
      " 'diningtable': 800,\n",
      " 'dog': 1598,\n",
      " 'horse': 803,\n",
      " 'motorbike': 801,\n",
      " 'person': 17401,\n",
      " 'pottedplant': 1202,\n",
      " 'sheep': 1084,\n",
      " 'sofa': 841,\n",
      " 'train': 704,\n",
      " 'tvmonitor': 893}\n",
      "Num classes (including bg) = 21\n"
     ]
    }
   ],
   "source": [
    "# bg 클래스 추가\n",
    "if 'bg' not in classes_count:\n",
    "    classes_count['bg'] = 0\n",
    "    class_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "C.class_mapping = class_mapping\n",
    "\n",
    "inv_map = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print('Num classes (including bg) = {}'.format(len(classes_count)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config has been written to config.pickle, and can be loaded when testing to ensure correct results\n",
      "17125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_output_filename = \"config.pickle\"\n",
    "\n",
    "with open(config_output_filename, 'wb') as config_f:\n",
    "    pickle.dump(C, config_f)\n",
    "    print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))\n",
    "\n",
    "random.shuffle(all_imgs)\n",
    "\n",
    "num_imgs = len(all_imgs)\n",
    "print(num_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': 'datasets/VOC2012/JPEGImages/2012_002636.jpg',\n",
       " 'width': 375,\n",
       " 'height': 500,\n",
       " 'bboxes': [{'class': 'person',\n",
       "   'x1': 8,\n",
       "   'x2': 354,\n",
       "   'y1': 82,\n",
       "   'y2': 271,\n",
       "   'difficult': False}],\n",
       " 'imageset': 'trainval'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_imgs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train samples 0\n",
      "Num val samples 0\n",
      "Num test samples 0\n"
     ]
    }
   ],
   "source": [
    "train_imgs = [s for s in all_imgs if s['imageset'] == 'train']\n",
    "val_imgs = [s for s in all_imgs if s['imageset'] == 'val']\n",
    "test_imgs = [s for s in all_imgs if s['imageset'] == 'test']\n",
    "\n",
    "print('Num train samples {}'.format(len(train_imgs)))\n",
    "print('Num val samples {}'.format(len(val_imgs)))\n",
    "print('Num test samples {}'.format(len(test_imgs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groundtruth anchor 데이터 가져오기\n",
    "data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\n",
    "data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='val')\n",
    "data_gen_test = data_generators.get_anchor_gt(test_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='val')\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    input_shape_img = (3, None, None)\n",
    "else:\n",
    "    input_shape_img = (None, None, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hasib/anaconda3/envs/p3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# input placeholder 정의\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "\n",
    "# base network(feature extractor) 정의 (resnet, VGG, Inception, Inception Resnet V2, etc)\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the RPN, built on the base layers\n",
    "# RPN 정의\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn = nn.rpn(shared_layers, num_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hasib/keras-frcnn/model/vgg16.h5.h5'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.base_net_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from /home/hasib/keras-frcnn/model/vgg16.h5\n",
      "Loaded pretrained weights!\n"
     ]
    }
   ],
   "source": [
    "# detection network 정의\n",
    "classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "try:\n",
    "    # load_weights by name\n",
    "    # some keras application model does not containing name\n",
    "    # for this kinds of model, we need to re-construct model with naming\n",
    "    print('loading weights from {}'.format(C.base_net_weights))\n",
    "    model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
    "    model_classifier.load_weights(C.base_net_weights, by_name=True)\n",
    "    print(\"Loaded pretrained weights!\")\n",
    "except:\n",
    "    print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
    "        https://github.com/fchollet/keras/tree/master/keras/applications')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing model summary....\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "roi_pooling_conv_3 (RoiPoolingC (None, 4, 7, 7, 512) 0           block5_conv3[0][0]               \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 4, 25088)     0           roi_pooling_conv_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 4, 4096)      102764544   time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 4, 4096)      0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 4, 4096)      16781312    time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "rpn_conv1 (Conv2D)              (None, None, None, 5 2359808     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 4, 4096)      0           time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "rpn_out_class (Conv2D)          (None, None, None, 9 4617        rpn_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "rpn_out_regress (Conv2D)        (None, None, None, 3 18468       rpn_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_class_21 (TimeDistributed (None, 4, 21)        86037       time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_regress_21 (TimeDistribut (None, 4, 80)        327760      time_distributed_15[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 137,057,234\n",
      "Trainable params: 137,057,234\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing model summary....\")\n",
    "print(model_all.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This has layers of number 30\n"
     ]
    }
   ],
   "source": [
    "print(\"This has layers of number\", len(model_all.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
